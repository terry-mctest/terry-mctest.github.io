In terms of logistics, my partner and I collaborated on this project using a two-branch github repo, each branch having its own .rmd file in which we were working; we communicated via email re: plans-of-attack, and kept each other posted on what additions/revisions/updates we were making in our respective .rmd files.

**What would you do differently?** I had this problem on a previous project and/or HW assignment, and it came up again on this project: I still do not quite have my head wrapped around how you pass variable names and/or text into an R custom function. I found a website that clued me in on using {{varname}} syntax (which did help a great deal), but this does not seem to work when, for example, you have a function that is generating plots/graphs, and you want to customize the title/labels based on the variable names that you are passing into the custom function (see for example the custom function named "explore" in our project3 .rmd). At any rate, one thing I would have done differently is to have already asked a question about this.

**What was the most difficult part for you?** I had (and continue to have) a bit of trouble navigating the dizzying array of models spelled out in sections 6 and 7 of the caret package documentation (https://topepo.github.io/caret/available-models.html and https://topepo.github.io/caret/train-models-by-tag.html, repectively). My troubles stem from the fact that (1) I have an extreme lack-of-knowledge about the vast majority of the models listed here available for use; (2) said lack-of-knowledge remains even after doing countless internet searches for countless numbers of models/methods (there were a handful of models/methods that didn't seem to show up at all when scouring the interent -- at least not by the name listed in the caret documentation -- but even for those models/methods for which I could find internet results, more often than not, my head was left spinning after reading the pages that I found on the topic); and (3) as a result, I was/am flying blind in terms of deciding what tuning parameters to use for a given model, and/or how to explain a particular method/model that I don't even really understand myself.

**What are your big take-aways from this project?** One big take-away from all this is what I allude to above; that is, that there are a dizzying array of machine-learning methods/models. Another take-away is that I'm left with general wonderings re: machine learning; for example, do machine-learning efforts really just turn a complete blind eye to trying to understand any of the underlying processes which might be related to the outcome which is being predicted? This feels akin to getting somewhere via a gps navigation system; that is, I don't really understand where I'm going or how I'm getting there (nor am I getting any better at understanding how my surroundings are laid out), I'm just blindly following orders. In other words, it almost feels like machine-learning is generating "hollow predictions", aka predictions devoid of any understanding. As I say, I don't know this to be the case, but I am left wondering if this is the case.

**Link to the nicely rendered repo:** (something like username.github.io/repo-name)

**Link to the usual github repo:** (something like github.com/username/repo-name)

