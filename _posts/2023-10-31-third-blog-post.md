At the risk of over-simplifying, it seems there are two broad categories of variable selection methods for building statistical models:  (1) selecting predictor variables based on theory / background knowledge, and (2) data-driven/machine learning-type selection methods (e.g. forward selection, backward selection).

Each of this week's recommended readings provides extensive detail regarding the relative weaknesses of the latter set of variable selection methods.  Among the points that most resonated within my own thick skull:
* Per Biom (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5969114/), iterative data-driven approaches have the following shortcomings (among others):  "First, unaccounted multiple testing will generally lead to underestimated p‐values. Second, p‐values for coefficient tests from a model do not test whether a variable is relevant per se, but rather whether it is relevant given the particular set of adjustment variables in that specific model."
* Per https://www.biostat.jhsph.edu/~iruczins/teaching/jf/ch10.pdf (also regarding iterative data-driven approaches): "Because of the one-at-a-time nature of adding/dropping variables, it’s possible to miss the optimal model."
* Per Ratner (https://link.springer.com/content/pdf/10.1057/jt.2009.26.pdf), stepwise approaches are "based on methods (for example, F tests) that were intended to be used to test pre-specified hypotheses."
* Also per Ratner, these approaches "prevent us from thinking about the problem."

In my mind at least, there are two questions that may help guide one towards one of these two large bins of variable selection methods:
* Do you know anything at all about the data with which you're working?
* Are you interested in using your model for prediction, or for making inference?

In theory, the answer to the first question should always be "yes".  For example, Ratner goes to great lengths to stress the importance of exploratory data analysis (EDA), and it should be possible for an analyst to conduct at least some sort of rudimentary EDA prior to building a model, even without having any prior background or training in the whatever theory might be relevant/applicable to the data with which one is working.  As for the latter question, even if one's answer is "prediction", I assume that "making inference" would be of at least some added benefit, as having a better understanding of the processes which affect that which you're predicting should make you better equipped to make predictions.

All that being said, IF one's answer to the first question above is somehow a hard "no", AND their answer to the second question above is somehow along the lines of "prediction to the complete exclusion of inference", then I assume there's nowhere left to turn other than data-driven/machine-learning-type variable selection methods (FWIW, I currently have very little experience with these methods, but from what I've read thus far, it seems that the "best subset" type of methods may be preferable, at least insofar as they're able to sidestep some of the knocks on the various stepwise methods).

Aside from this rare (perhaps non-existent?) circumstance, it seems that you'd want to use at least some non-machine discretion in terms of what predictors are entered into your model.  At a minimum, this non-machine (i.e. human) discretion should result in a more-interpretable model relative to one that built from completely data-driven selections.  Moreover, as Ratner points out, "EDA allows the statistician to assess whether or not a given variable, say, X, needs a transformation/re-expression (for example, log(X), sin(X) or 1/X).  The traditional variable selection methods
cannot achieve such transformations or a priori construction of new variables based on the original variables; 2 and this inability is a serious weakness of the variable selection methodology."
